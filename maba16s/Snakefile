import time
import os
from shutil import copy2
import pathlib
import pandas as pd

# workflow start: set up output directories & save the config files
configfile: "config/config.yaml"

OUTDIR = config['parameters']['outdir'] + "/"
configfile: OUTDIR + "config/config_good_samples.yaml"
SAMPLES = config['GOODSAMPLES']

onstart:
    print("This is MABA16S: analysing high quality samples")
    time.sleep(1)

    # copy the config file to output dir
    pathlib.Path(OUTDIR).mkdir(parents=True, exist_ok=True)
    copy2('config/config.yaml', OUTDIR)
    for i in SAMPLES.items():
        print(i[0], '\t', i[1])
    print(f'output directory is: {OUTDIR}')

# Error handling
onerror:
    print(f"these were the samples: {SAMPLES}")
    print("error has occured")

# Define local rules
localrules: all

# Master rule
rule all:
    input:
        #expand(OUTDIR + 'BLAST/{sample}', sample=SAMPLES),
        #expand(OUTDIR + "reports/{sample}.xlsx", sample=SAMPLES),
        expand(OUTDIR + "sankeys/{sample}_sankey.html", sample=SAMPLES)

# Build the blast database
rule build_blast_db:
    input:
        "db/silva/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
    output:
        directory("db/blastDB")
    threads: 1
    conda:
        "envs/blast.yaml"
    log:
        "log/blastbuild_db.log"
    params:
        "db/blastDB"
    shell:
        "makeblastdb -in {input} -dbtype nucl -out {output}/blastDB 2> {log}"

# Split the fastq file based on taxonomic rank assignment
rule genus_read_extract:
    input:
        report = OUTDIR + "kraken2/{sample}/krakenreport_filtered.txt",
        krakenfile = OUTDIR + "kraken2/{sample}/output.txt",
        fastq = OUTDIR + "filtered_reads/{sample}.fastq.gz"
    output:
        directory(OUTDIR + "genus_reads/{sample}/")
    threads:
        1
    conda:
        "envs/kraken2.yaml"
    log:
        OUTDIR + "log/{sample}/genusreadextract.log"
    shell:
        '''
        python scripts/krakenextract.py {input.report} {input.krakenfile} {input.fastq} {output}
        '''

# Map genus-specific reads to reference and generate consensus FASTA
rule map_genera:
    input:
        readdir = rules.genus_read_extract.output,
        ref = "db/silva/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
    output:
        ref_dir = directory(OUTDIR + "kraken2consensus/{sample}/reference_fastas/"),
        aligned_dir=directory(OUTDIR + "kraken2consensus/{sample}/aligned_reads/"),
        consensus_dir=directory(OUTDIR + "kraken2consensus/{sample}/consensus_fastas/")
    params:
        outdir = OUTDIR + "kraken2consensus/{sample}/"
    threads: 4
    conda:
        "envs/minimapsamtools.yaml"
    log:
        OUTDIR + "log/{sample}/map_genera.log"
    shell:
        "python scripts/aligner.py {input.readdir} {input.ref} {params.outdir} 2> {log}"

# Run BLASTn on consensus FASTA files
rule blast_consensus_genera:
    input:
        db = rules.build_blast_db.output,
        fastas = rules.map_genera.output.consensus_dir
    output:
        outdir = directory(OUTDIR + 'BLAST/{sample}')
    threads:
        4
    conda:
        "envs/blast.yaml"
    log:
        OUTDIR + 'log/{sample}/blast_consensus_genera.log'
    shell:
        "python scripts/blast_consensus.py {input.fastas} {threads} {input.db} {output.outdir} 2> {log} "


rule generate_reports:
    input:
        blast = rules.blast_consensus_genera.output,
        readcount = OUTDIR + "kraken2/{sample}/krakenreport_filtered.txt"
    output:
        OUTDIR + "reports/{sample}.xlsx"
    threads: 1
    conda:
        "envs/kraken2.yaml" # this env has pandas
    log:
        OUTDIR + "log/{sample}/report.log"
    shell:
        "python scripts/generate_reports.py {input.blast} {output} {input.readcount} 2> {log}"

rule make_sankeys:
    input:
        rules.generate_reports.output
    output:
        OUTDIR + "sankeys/{sample}_sankey.html"
    threads: 1 
    conda:
        "envs/plotly.yaml"
    log:
        OUTDIR + "log/make_sankey/{sample}.log"
    shell:
        "python scripts/sankey_report.py {input} {output}"