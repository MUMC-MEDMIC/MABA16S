"""
MABA16S Analysis Pipeline
==========================

Description:
------------
This Snakemake pipeline performs an in-depth analysis of high-quality samples 
from 16S metagenomic sequencing. It integrates Kraken2, Minimap2, 
BLASTn, and custom Python scripts to characterize and visualize the taxonomic 
composition of samples.

Workflow:
---------
1. Extract genus-specific reads from Kraken2 filtered reports.
2. Map extracted reads to reference sequences and generate consensus FASTA files.
3. Perform BLASTn analysis on consensus FASTA files against a reference database.
4. Generate detailed Excel reports summarizing BLASTn and Kraken2 results.
5. Create Sankey diagrams visualizing the taxonomic composition.

Main Rules:
-----------
- `build_blast_db`: Builds a BLAST database from a SILVA reference file.
- `genus_read_extract`: Extracts genus-specific reads based on Kraken2 results.
- `map_genera`: Maps genus-specific reads to references and generates consensus sequences.
- `blast_consensus_genera`: Runs BLASTn on consensus FASTA files.
- `generate_reports`: Produces Excel reports summarizing results.
- `make_sankeys`: Creates Sankey diagrams for visualizing results.
"""

import time
import os
from shutil import copy2
import pathlib
import pandas as pd

# workflow start: set up output directories & save the config files
configfile: "config/config.yaml"

OUTDIR = config['parameters']['outdir'] + "/"
configfile: OUTDIR + "config/config_good_samples.yaml"
SAMPLES = config['GOODSAMPLES']

onstart:
    print("This is MABA16S: analysing high quality samples")
    time.sleep(1)

    # copy the config file to output dir
    pathlib.Path(OUTDIR).mkdir(parents=True, exist_ok=True)
    copy2('config/config.yaml', OUTDIR)
    for i in SAMPLES.items():
        print(i[0], '\t', i[1])
    print(f'output directory is: {OUTDIR}')

# Error handling
onerror:
    print(f"Attempted to analyse these samples: {SAMPLES}")
    print("An error has occured")

# Define local rules
localrules: all, generate_reports, make_sankeys, post_analysis_QC

# Master rule
rule all:
    input:
        expand(OUTDIR + "sankeys/{sample}_sankey.html", sample=SAMPLES),
        expand(OUTDIR + "QC/{sample}/{sample}_qcPostAnalysis.txt", sample=SAMPLES)

# Build the blast database
rule build_blast_db:
    input:
        "db/silva/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
    output:
        directory("db/blastDB")
    threads: 1
    conda:
        "envs/blast.yaml"
    log:
        "log/blastbuild_db.log"
    params:
        "db/blastDB"
    shell:
        "makeblastdb -in {input} -dbtype nucl -out {output}/blastDB 2> {log}"

# Split the fastq file based on taxonomic rank assignment
rule genus_read_extract:
    input:
        report = OUTDIR + "kraken2/{sample}/krakenreport_filtered.txt",
        krakenfile = OUTDIR + "kraken2/{sample}/output.txt",
        fastq = OUTDIR + "filtered_reads/{sample}.fastq.gz"
    output:
        directory(OUTDIR + "genus_reads/{sample}/")
    threads:
        1
    conda:
        "envs/kraken2.yaml"
    log:
        OUTDIR + "log/{sample}/genusreadextract.log"
    shell:
        '''
        python scripts/krakenextract.py {input.report} {input.krakenfile} {input.fastq} {output}
        '''

# Map genus-specific reads to reference and generate consensus FASTA
rule map_genera:
    input:
        readdir = rules.genus_read_extract.output,
        ref = "db/silva/data/SILVA_138.1_SSURef_NR99_tax_silva.fasta"
    output:
        ref_dir = directory(OUTDIR + "kraken2consensus/{sample}/reference_fastas/"),
        aligned_dir=directory(OUTDIR + "kraken2consensus/{sample}/aligned_reads/"),
        consensus_dir=directory(OUTDIR + "kraken2consensus/{sample}/consensus_fastas/")
    params:
        outdir = OUTDIR + "kraken2consensus/{sample}/"
    threads: 4
    conda:
        "envs/minimapsamtools.yaml"
    log:
        OUTDIR + "log/{sample}/map_genera.log"
    shell:
        "python scripts/aligner.py {input.readdir} {input.ref} {params.outdir} 2> {log}"

# Run BLASTn on consensus FASTA files
rule blast_consensus_genera:
    input:
        db = rules.build_blast_db.output,
        fastas = rules.map_genera.output.consensus_dir
    output:
        outdir = directory(OUTDIR + 'BLAST/{sample}')
    threads:
        4
    conda:
        "envs/blast.yaml"
    log:
        OUTDIR + 'log/{sample}/blast_consensus_genera.log'
    shell:
        "python scripts/blast_consensus.py {input.fastas} {threads} {input.db} {output.outdir} 2> {log} "

# Post-analysis QC (checking)
rule post_analysis_QC:
    input:
        qc = OUTDIR + "QC/{sample}/{sample}_qcPreprocessing.txt",
        genus_reads = rules.genus_read_extract.output,
        alignment = rules.map_genera.output.aligned_dir, 
        consensus = rules.map_genera.output.consensus_dir,
        blast = rules.blast_consensus_genera.output
    output:
        OUTDIR + "QC/{sample}/{sample}_qcPostAnalysis.txt"
    log:
        OUTDIR + "log/{sample}/qcPostAnalysis.txt"
    shell:
        """
        # Read the original QC file into a temporary variable
        qcData=$(cat {input.qc})

        # Count the number of files in the genus reads directory
        nGenusReads=$(ls -1 {input.genus_reads} | wc -l)

        # Count the number of files in the aligned reads directory
        nAligned=$(ls -1 {input.alignment} | wc -l)

        # Count the number of files in the consensus FASTA directory
        nConsensusFastas=$(ls -1 {input.consensus} | wc -l)

        # Count the number of files in the BLAST directory
        nBlast=$(ls -1 {input.blast} | wc -l)

        # Write the extended QC data to the new file
        echo -e "sampleID\trawReads\tfilteredReads\tnGenera\tnGenusReads\tnAligned\tnConsensusFastas\tnBlast" > {output}
        echo -e "${{qcData}}\t${{nGenusReads}}\t${{nAligned}}\t${{nConsensusFastas}}\t${{nBlast}}" >> {output} 2> {log}
        """

# Create a basic report per sample
rule generate_reports:
    input:
        blast = rules.blast_consensus_genera.output,
        readcount = OUTDIR + "kraken2/{sample}/krakenreport_filtered.txt"
    output:
        OUTDIR + "reports/{sample}.xlsx"
    threads: 1
    conda:
        "envs/kraken2.yaml" # this env has pandas
    log:
        OUTDIR + "log/{sample}/report.log"
    shell:
        "python scripts/generate_reports.py {input.blast} {output} {input.readcount} 2> {log}"

# create sankey plots per sample
rule make_sankeys:
    input:
        rules.generate_reports.output
    output:
        OUTDIR + "sankeys/{sample}_sankey.html"
    threads: 1 
    conda:
        "envs/plotly.yaml"
    log:
        OUTDIR + "log/make_sankey/{sample}.log"
    shell:
        "python scripts/sankey_report.py {input} {output}"